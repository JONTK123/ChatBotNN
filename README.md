# ðŸ§  Token-Predict-API â€“ Neuralâ€‘Network TokenPredictor

A **tiny, visual LLM playground** built with **PyTorch**, **FastAPI** and vanilla **JavaScript**.  
It *predicts the next token (word)* given the **last 4 tokens** of a sentence and streams all training internals to the browser in realâ€‘time.

> **Educational only** â€“ perfect for demos, live coding, and understanding how languageâ€‘model pieces fit together.

---

## ðŸŽ¯  Purpose

*   Deâ€‘mystify how *small* LLMs learn tokenâ€‘byâ€‘token  
*   Visualise **embeddings, activations, weight deltas** while training  
*   Mix **handâ€‘made tokenisation** with a real **GPTâ€‘2 tokenizer**  
*   Provide a hackable codeâ€‘base for experiments and blog posts  

---

## ðŸ“¦  Features

| Category | Details                                                                        |
|----------|--------------------------------------------------------------------------------|
| **Model** | Custom `MiniGPT` (embeddings â†’ 1 or 2 hidden layers â†’ vocab logits)            |
| **Visuals** | Live loss lineâ€‘chart, 3â€‘D loss surface, confusion matrix, topâ€‘error tokens     |
| **Frontend** | Plain HTML+ JS (Plotly, Cytoscape.js, WebSockets)                              |
| **Training** | Adam+CrossEntropy, configurable epochs, live batch streaming                   |
| **Tokeniser** | GPTâ€‘2 tokenizer (**HuggingFace**) + manual split for comparison                |
| **API** | FastAPI endpoints `/treinar`, `/tokenizar`, `/completar`, plus WebSocket `/ws` |

---

## âš ï¸  Limitations

* Predicts **only the next token** â€“ no full text generation  
* Context window hardâ€‘coded to **4 tokens**  
* Needs **â‰¥ 50 phrases** and around **100â€‘150 epochs** for stable learning  
* Frontend is deliberately **minimal** (studyâ€‘oriented)  
* Not productionâ€‘ready; expect mistakes for rare / unseen tokens  

---

## ðŸ’¡  Ideas for Improvement

*   Extend context to 6â€‘8 tokens  
*   Wider embeddings / hidden layers  
*   Larger, more diverse training corpora  
*   Replace FFâ€‘layers with GRU / Transformer blocks  
*   Generate full sentences instead of tokenâ€‘byâ€‘token  

---

## ðŸ§­  System Flow

1. Paste training phrases â†’ click **â€œTokenise!â€**  
2. App shows both **split tokens** and **GPTâ€‘2 tokens**  
3. Click **â€œTrainâ€** â†’ live graphs & activations stream via WebSocket  
4. When done, type a 4â€‘token prompt and press **â€œCompleteâ€**  
5. API returns the most probable next token, rendered instantly  

---

## âœ¨  Example Prompts

| Prompt (`last 4 tokens`) | Likely Prediction |
|--------------------------|-------------------|
| `eu gosto de comer`      | `maÃ§Ã£`, `banana`, â€¦ |
| `hoje o cÃ©u estÃ¡`        | `limpo`, `nublado`, â€¦ |
| `vocÃª precisa estudar`   | `mais`, `agora`, â€¦ |
| `ela gosta de pintar`    | `quadros`, `paredes` |

---

## ðŸ“‚  Mini Dataset (you can swap your own - at any language!!!!!!!)

```python
frases = [
    "eu gosto de comer maÃ§Ã£",
    "ela gosta de pintar quadros",
    "nÃ³s vamos ao parque amanhÃ£",
    "ele vai sair hoje Ã  tarde",
    "vocÃª precisa estudar agora",
    "hoje o cÃ©u estÃ¡ limpo",
    "amanhÃ£ o tempo estarÃ¡ frio",
    "nÃ³s queremos comprar uma casa",
    "vocÃª quer comprar um celular",
    "eles vÃ£o ao mercado cedo",
]
```

## ðŸ› Installation & Usage

###1â€“Clone

```python
git clone https://github.com/JONTK123/Token-Predict-API.git
cd ChatBotNN
```

### 2 â€“ Dependencies

```python
python -m venv .venv
# Windows  â†’  .venv\Scripts\activate
source .venv/bin/activate
pip install -r requirements.txt
```

### 3 â€“ Start the API

```python
uvicorn backend.nn:app --reload
```

### 4 â€“ Open the Frontâ€‘End

Simply open frontend/index.html in your browser.
Paste sentences â†’ Tokenise â†’ Train â†’ play with predictions

###5 - Requirements.txt

```python
fastapi
uvicorn
torch
transformers
scikit-learn
matplotlib
numpy
anyio
```

## ðŸ”How It Works

1. Tokenisation â€“ GPTâ€‘2 tokenizer (HuggingFace) + naive .split() for sideâ€‘byâ€‘side comparison
2. Pair generation â€“ for every sentence:
Input = first N tokens â†’ Label = the next token
3. Model â€“ embeddings â†’ hidden layer(s) â†’ vocabâ€‘size logits 
4. Training loop â€“ Adam + CrossEntropy, live progress pushed via WebSocket
5. Inference â€“ give 4 tokens, API returns argmax(logits) as the predicted next token

## ðŸ§  About

Built by [@Thiago / JONTK123] to learn & teach:
Linkedin -> https://www.linkedin.com/in/thiago-luiz-fossa-26b440276/?locale=pt_BR

- Neuralâ€‘network fundamentals
- Embedding layers and tokenisation quirks
- PyTorch training pipelines
- Miniâ€‘LLM behaviour on tiny datasets
## ðŸš§ Disclaimer

This repository **is not production-ready**.  
Predictions may be wrong or biased, especially for unseen words.  
It exists purely for learning, experimentation, and visual intuition.

Pull requests, issues, and discussions are welcome â€“ have fun.